{
	"name": "Process Crawled Data",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "SparkPoolManos",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "88341739-6d59-413d-ac52-2d26d220c1a2"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/5521848d-954c-480a-9e4b-879a55f9ff16/resourceGroups/MantinadesCrawlerRG/providers/Microsoft.Synapse/workspaces/mantinades-crawler-workspace/bigDataPools/SparkPoolManos",
				"name": "SparkPoolManos",
				"type": "Spark",
				"endpoint": "https://mantinades-crawler-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPoolManos",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.4",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# NOTE: Synapse does not support azure-identity\n",
					"import ast\n",
					"import os\n",
					"import math\n",
					"import json\n",
					"import numpy as np\n",
					"import pandas as p\n",
					"from io import BytesIO, StringIO\n",
					"\n",
					"from azure.storage.blob import BlobServiceClient\n",
					"from azure.core.exceptions import ResourceExistsError\n",
					"from notebookutils import mssparkutils\n",
					"\n",
					"import pandas as pd\n",
					"from azure.storage.blob import BlobServiceClient\n",
					"\n",
					"# Get storage key from Azure Key Vault\n",
					"storage_key = json.loads(mssparkutils.credentials.getPropertiesAll(\"mantinadescrawleraccount\"))['AuthKey']\n",
					"\n",
					"\n",
					"\n",
					"def get_df_from_container_blob(container_client, blob_name):\n",
					"    # Get blob client from container client\n",
					"    blob_client = container_client.get_blob_client(blob_name)\n",
					"\n",
					"    # Download blob content\n",
					"    stream = BytesIO(blob_client.download_blob().readall())\n",
					"\n",
					"    # Read CSV into DataFrame\n",
					"    df = pd.read_csv(stream, sep='|', header=0)\n",
					"\n",
					"    return df\n",
					"\n",
					"\n",
					"\n",
					"def write_df_to_container_blob(df, container_client, blob_name):\n",
					"    # Convert DataFrame to Parquet in memory\n",
					"    parquet_buffer = BytesIO()\n",
					"    df.to_parquet(parquet_buffer, index=False)\n",
					"    parquet_buffer.seek(0)\n",
					"\n",
					"    blob_client = container_client.get_blob_client(blob_name)\n",
					"    blob_client.upload_blob(parquet_buffer, overwrite=True)\n",
					"\n",
					"    return\n",
					"\n",
					"\n",
					"account_url = \"https://mantinadescrawleraccount.blob.core.windows.net\"\n",
					"\n",
					"# Create the BlobServiceClient object\n",
					"blob_service_client = BlobServiceClient(account_url, credential=storage_key)"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"source": [
					"CRAWLED_DATA_CONTAINER   = 'data'\n",
					"PROCESSED_DATA_CONTAINER = 'processed-data'\n",
					"\n",
					"# Get the crawled data container client\n",
					"crawled_data_container_client = blob_service_client.get_container_client(CRAWLED_DATA_CONTAINER)\n",
					"\n",
					"# Get or create the processed data container client\n",
					"# Create the container\n",
					"try:\n",
					"    processed_data_container_client = blob_service_client.create_container(PROCESSED_DATA_CONTAINER)\n",
					"except ResourceExistsError:\n",
					"    print('Container data already exists')\n",
					"    processed_data_container_client = blob_service_client.get_container_client(PROCESSED_DATA_CONTAINER)"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"run_control": {
						"frozen": false
					},
					"editable": true
				},
				"source": [
					"\"\"\"\n",
					"NOTE: I had two choices.\n",
					"\n",
					"1. Do all with Python and then have postgres load the resulting files\n",
					"one for each DIM and FACT.\n",
					"\n",
					"2. Load the result of read_and_unfold_crawled_data into an SQL Staging table\n",
					"and then make the DIM and FACT tables using SQL.\n",
					"\n",
					"Currently I will do the 1st and because 2nd is more scalable I'll do it in\n",
					"the future\n",
					"\"\"\"\n",
					"def read_and_unfold_crawled_data():\n",
					"    crawled_data_filenames = crawled_data_container_client.list_blob_names()\n",
					"\n",
					"    all_mantinades_df = None\n",
					"\n",
					"    for filename in crawled_data_filenames:\n",
					"        if filename.endswith('.json'):\n",
					"            continue\n",
					"\n",
					"        curr_df = get_df_from_container_blob(crawled_data_container_client, filename)\n",
					"\n",
					"        if all_mantinades_df is None:\n",
					"            all_mantinades_df = curr_df\n",
					"        else:\n",
					"            all_mantinades_df = pd.concat([all_mantinades_df, curr_df], axis=0,\n",
					"                                          ignore_index=True)\n",
					"\n",
					"    all_mantinades_df['topics'] = all_mantinades_df['topics'].apply(lambda x: ast.literal_eval(x))\n",
					"    all_mantinades_df = all_mantinades_df.explode('topics', ignore_index=True)\n",
					"\n",
					"    # TODO: CHECK IN TOPICS THAT NOTHING IS NAN\n",
					"    return all_mantinades_df\n",
					"\n",
					"\n",
					"all_mantinades_df = read_and_unfold_crawled_data()\n",
					"\n",
					"print(all_mantinades_df)"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# NOTE: DO NOT FORGET THE UNKNOWN ROW IN ALL DIMS\n",
					"def get_dim_topic_rows(mantinades_df):\n",
					"    topics = list(mantinades_df['topics'].unique())\n",
					"    topics = ['Άγνωστο'] + topics\n",
					"    topics = list(enumerate(topics))\n",
					"\n",
					"    topic_to_id_map = {t: i for i, t in topics}\n",
					"\n",
					"    write_df_to_container_blob(pd.DataFrame(topics, columns=['Key', 'Name']), \n",
					"                               processed_data_container_client,\n",
					"                               'DIM_Topic.parquet')\n",
					"\n",
					"    return topic_to_id_map\n",
					"\n",
					"\n",
					"\n",
					"\n",
					"topic_to_id_map         = get_dim_topic_rows(all_mantinades_df)\n",
					"print(topic_to_id_map)"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def get_dim_author_rows(mantinades_df):\n",
					"    authors = list(mantinades_df['author'].unique())\n",
					"    authors = ['Άγνωστο'] + authors\n",
					"    authors = [a for a in authors if type(a) == str]\n",
					"    authors = list(enumerate(authors))\n",
					"\n",
					"    author_to_id_map = {a: i for i, a in authors}\n",
					"\n",
					"    write_df_to_container_blob(pd.DataFrame(authors, columns=['Key', 'Name']), \n",
					"                               processed_data_container_client,\n",
					"                               'DIM_Author.parquet')\n",
					"\n",
					"    return author_to_id_map\n",
					"\n",
					"\n",
					"\n",
					"author_to_id_map        = get_dim_author_rows(all_mantinades_df)\n",
					"print(author_to_id_map)"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def get_dim_mantinada_rows(mantinades_df):\n",
					"    dim_mantinada_df = mantinades_df[['id', 'text']].drop_duplicates()\n",
					"    dim_mantinada_df.columns = ['ID', 'Content']\n",
					"\n",
					"    dim_mantinada_df['Key'] = list(range(1, len(dim_mantinada_df) + 1))\n",
					"\n",
					"    dim_mantinada_df = dim_mantinada_df[['Key', 'ID', 'Content']]\n",
					"\n",
					"    mantinada_id_to_key_map = {r['ID']: r['Key'] for _, r in dim_mantinada_df.iterrows()}\n",
					"\n",
					"    write_df_to_container_blob(dim_mantinada_df, \n",
					"                               processed_data_container_client,\n",
					"                               'DIM_Mantinada.parquet')\n",
					"\n",
					"    return mantinada_id_to_key_map\n",
					"\n",
					"\n",
					"mantinada_key_to_id_map = get_dim_mantinada_rows(all_mantinades_df)\n",
					"print(mantinada_key_to_id_map)"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def get_dim_date_rows(mantinades_df):\n",
					"    start_date = '01-01-1950'\n",
					"    end_date = '01-01-2100'\n",
					"\n",
					"    date_range = pd.date_range(start_date, end_date, freq='D')\n",
					"\n",
					"    dim_date_data = {\n",
					"        'Key':                  date_range.strftime('%Y%m%d').tolist(),\n",
					"        'Date_Full':            date_range.strftime('%d/%m/%Y').tolist(),\n",
					"        'Date_Full_US_Format':  date_range.strftime('%m/%d/%Y').tolist(),\n",
					"        'Day':                  date_range.day.tolist(),\n",
					"        'Day_Name':             date_range.day_name().tolist(),\n",
					"        'Month':                date_range.month.tolist(),\n",
					"        'Month_Name':           date_range.month_name().tolist(),\n",
					"        'Year':                 date_range.year.tolist(),\n",
					"        'Day_of_Year':          date_range.day_of_year.tolist(),\n",
					"        'Day_of_Week':          date_range.day_of_week.tolist(),\n",
					"        'Quarter':              date_range.quarter.tolist(),\n",
					"        'Is_Month_Start':       date_range.is_month_start.tolist(),\n",
					"        'Is_Month_End':         date_range.is_month_end.tolist(),\n",
					"        'Is_Quarter_Start':     date_range.is_quarter_start.tolist(),\n",
					"        'Is_Quarter_End':       date_range.is_quarter_end.tolist(),\n",
					"        'Is_Year_Start':        date_range.is_year_start.tolist(),\n",
					"        'Is_Year_End':          date_range.is_year_end.tolist(),\n",
					"        'Is_Leap_Year':         date_range.is_leap_year.tolist()\n",
					"    }\n",
					"\n",
					"    dim_date_data_df = pd.DataFrame(dim_date_data)\n",
					"\n",
					"    write_df_to_container_blob(dim_date_data_df, \n",
					"                               processed_data_container_client,\n",
					"                               'DIM_Date.parquet')\n",
					"    return\n",
					"\n",
					"\n",
					"get_dim_date_rows(all_mantinades_df)"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def get_fact_mantinada_entry_rows(mantinada_df, topic_to_id_map, author_to_id_map,\n",
					"                                  mantinada_key_to_id_map):\n",
					"    mantinada_entry_df = pd.DataFrame()\n",
					"\n",
					"    mantinada_entry_df['Key'] = list(range(1, len(mantinada_df) + 1))\n",
					"    mantinada_entry_df['DIM_Topic_Key'] = mantinada_df['topics'].apply(lambda x: topic_to_id_map[x])\n",
					"\n",
					"    mantinada_df['author'] = mantinada_df['author'].apply(lambda x: x if type(x) == str else 'Άγνωστο')\n",
					"    mantinada_entry_df['DIM_Author_Key'] = mantinada_df['author'].apply(lambda x: author_to_id_map[x])\n",
					"\n",
					"    mantinada_entry_df['DIM_Date_Key'] = mantinada_df['date'].apply(lambda x: ''.join(x.split('/')[::-1]))\n",
					"    mantinada_entry_df['DIM_Mantinada_Key'] = mantinada_df['id'].apply(lambda x: mantinada_key_to_id_map[x])\n",
					"\n",
					"    write_df_to_container_blob(pd.DataFrame(mantinada_entry_df), \n",
					"                               processed_data_container_client,\n",
					"                               'FACT_Mantinada_Entry.parquet')\n",
					"\n",
					"    return\n",
					"\n",
					"get_fact_mantinada_entry_rows(all_mantinades_df, topic_to_id_map,\n",
					"                              author_to_id_map, mantinada_key_to_id_map)\n",
					"print(all_mantinades_df)"
				],
				"execution_count": 8
			}
		]
	}
}